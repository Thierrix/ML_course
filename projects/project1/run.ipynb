{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from cleaning_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Id'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"data/dataset/dataset\"\n",
    "x_train, x_test, y_train, train_ids, test_ids, y_test, labels =  load_csv_data(DATA_PATH, sub_sample=False)\n",
    "labels.pop(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key_by_value(d, value):\n",
    "    for key, val in d.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from utils import remove_features, find_key_by_value\n",
    "\n",
    "#Defining some constants\n",
    "ACCEPTABLE_NAN_PERCENTAGE = 0.3\n",
    "dictionary_features = {\n",
    "    **dict.fromkeys(['_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR',\n",
    "                     'DISPCODE', 'SEQNO', '_PSU', 'SEX', 'QSTVER', '_STSTR',\n",
    "                     '_STRWT', '_RAWRAKE', '_WT2RAKE', '_DUALUSE', '_LLCPWT',\n",
    "                     '_DRDXAR1', '_RACE_G1', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4', '_BMI5', '_BMI5CAT',\n",
    "                     'FTJUDA1_', 'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_MISFRTN', '_MISVEGN',\n",
    "                     '_FRTRESP', '_VEGRESP', '_FRUTSUM', '_FRUTSUM', '_FRT16', '_VEG23', '_FRUITEX',\n",
    "                     '_VEGETEX', 'QSTLANG', 'FRUTDA1_', '_VEGESUM'\n",
    "                   \n",
    "                    ], {}),\n",
    "    \n",
    "    **dict.fromkeys(['ALCDAY5', 'STRENGTH'], {'dont_know_not_sure' : 777, 'none' : 888, 'refused' : 999}),\n",
    "    \n",
    "    **dict.fromkeys(['PHYSHLTH', 'MENTHLTH'], {'dont_know_not_sure' : 77, 'none' : 88, 'refused' : 99}),\n",
    "    \n",
    "    **dict.fromkeys(['CHILDREN'], {'none' : 88, 'refused' : 99}),\n",
    "    \n",
    "     **dict.fromkeys(['INCOME2', '_PRACE1', '_MRACE1'], {'dont_know_not_sure' : 77, 'refused' : 99}),\n",
    "\n",
    "    **dict.fromkeys(['FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVORANG', 'FVGREEN', 'VEGETAB1'], {'never' : 555, 'dont_know_not_sure' : 777, 'refused' : 999}),\n",
    "    \n",
    "     **dict.fromkeys(['GENHLTH', 'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'BPHIGH4',\n",
    "                      'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', 'ASTHMA3',\n",
    "                      'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2',\n",
    "                      'CHCKIDNY', 'DIABETE3', 'RENTHOM1', 'VETERAN3', 'INTERNET',\n",
    "                     'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES',\n",
    "                      'DIFFALON', 'SMOKE100', 'USENOW3', 'EXERANY2', 'FLUSHOT6', 'PNEUVAC3',\n",
    "                      'HIVTST6', 'DRNKANY5'\n",
    "                     ], {'dont_know_not_sure' : 7, 'refused' : 9}),\n",
    "    \n",
    "    **dict.fromkeys(['CHECKUP1', 'SEATBELT'], {'dont_know_not_sure' : 7, 'never' : 8, 'refused' : 9}),\n",
    "    \n",
    "    **dict.fromkeys(['MARITAL', 'EDUCA', 'EMPLOY1', '_CHISPNC', '_RFHLTH',\n",
    "                      '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL', '_LTASTH1',\n",
    "                      '_CASTHM1', '_ASTHMS1', '_HISPANC', '_RACE', '_RACEG21',\n",
    "                      '_RACEGR3', '_RFBMI5', '_CHLDCNT', '_EDUCAG', '_INCOMG',\n",
    "                      '_SMOKER3', '_RFSMOK3', '_RFBING5', '_RFDRHV5', '_FRTLT1',\n",
    "                     '_VEGLT1', '_TOTINDA', 'PAMISS1_', '_PACAT1', '_PAINDX1', '_PA150R2',\n",
    "                     '_PA300R2', '_PA30021', '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1',\n",
    "                     '_LMTWRK1', '_LMTSCL1', '_RFSEAT2', '_RFSEAT3', '_AIDTST3'\n",
    "                     ], {'refused' : 9}),\n",
    "    \n",
    "    **dict.fromkeys(['HEIGHT3', 'WEIGHT2'], {'dont_know_not_sure' : 7777, 'refused' : 9999}),\n",
    "    \n",
    "    **dict.fromkeys(['_AGEG5YR'], {'refused' : 14}),\n",
    "\n",
    "    **dict.fromkeys(['_AGE65YR'], {'refused' : 3}),\n",
    "\n",
    "    **dict.fromkeys(['WTKG3'], {'refused' : 99999}),\n",
    "    **dict.fromkeys(['DROCDY3_'], {'refused' : 900}),\n",
    "    **dict.fromkeys(['_DRNKWEK'], {'refused' : 99900}),\n",
    "     **dict.fromkeys(['FC60_', 'MAXVO2_'], {'refused' : 999}),\n",
    "     **dict.fromkeys(['STRFREQ_'], {'refused' : 99}),\n",
    "   \n",
    "}\n",
    "\n",
    "category_features = {'categorical' : ['HLTHPLN1','_STATE','FMONTH','IDATE', 'IMONTH', 'DISPCODE', 'PERSDOC2',\n",
    "                             'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3',\n",
    "                            'ASTHMA3', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2', 'CHCKIDNY'\n",
    "                             'DIABETE3','SEX', 'MARITAL', 'EDUCA', 'RENTHOM1', 'VETERAN3', 'EMPLOY1', 'INCOME2', 'INTERNET', 'QLACTLM2',\n",
    "                             'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'USENOW3', 'EXERANY2', 'SEATBELT',\n",
    "                             'FLUSHOT6', 'PNEUVAC3', 'HIVTST6', 'QSTVER', 'QSTLANG','_CHISPNC', '_DUALUSE', '_RFHLTH', '_HCVU651', '_RFHYPE5',\n",
    "                             '_CHOLCHK', '_RFCHOL', '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR1', '_PRACE1', '_MRACE1', '_HISPANC', '_RACE',\n",
    "                             '_RACEG21', '_RACEGR3',  '_RACE_G1', '_AGEG5YR', '_AGE65YR', '_AGE_G', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG',\n",
    "                             '_INCOMG', '_SMOKER3', '_RFSMOK3', 'DRNKANY5', '_RFBING5', '_RFDRHV5', '_MISFRTN', '_MISVEGN', '_FRTRESP', '_VEGRESP',\n",
    "                             '_FRTLT1', '_VEGLT1', '_FRT16', '_VEG23', '_FRUITEX', '_VEGETEX', '_TOTINDA', 'PAMISS1_', '_PACAT1', '_PAINDX1',\n",
    "                             '_PA150R2', '_PA300R2', '_PA30021', '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1', '_LMTWRK1', '_LMTSCL1',\n",
    "                             '_RFSEAT2', '_RFSEAT3', '_AIDTST3', 'IYEAR', '_RFSMOK3'\n",
    "                             \n",
    "                             \n",
    "                            ],\n",
    "\n",
    "            \n",
    "            'continuous' : ['WEIGHT2', 'HEIGHT3', 'ALCDAY5', 'FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVGREEN',\n",
    "                            'FVORANG', 'VEGETAB1', 'STRENGTH', '_AGE80', 'HTIN4', 'WTKG3', 'HTM4', '_BMI5', 'DROCDY3_',\n",
    "                           '_DRNKWEK', 'FTJUDA1_', 'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_FRUTSUM', '_FRUTSUM',\n",
    "                           'MAXVO2_', 'FC60_', 'STRFREQ_', 'PHYSHLTH', 'IDAY', 'CHILDREN'],\n",
    "            \n",
    "            'not_sure' : ['SEQNO', '_PSU' , '_STSTR', '_STRWT', '_RAWRAKE', ' _WT2RAKE', '_LLCPWT']\n",
    "           }\n",
    "\n",
    "def clean_data_x(x_train, labels):\n",
    "    \"\"\"\n",
    "    Cleaning data\n",
    "    :param x_train: training data\n",
    "    :return: cleaned data\n",
    "    \"\"\"\n",
    "    #Removing the first label which is the id\n",
    "    features_number = x_train.shape[1]\n",
    "    features = list(dict.fromkeys(labels))  \n",
    "    features = [features[i]  for i in range(features_number)]\n",
    "    features = {word: index for index, word in enumerate(features)}\n",
    "\n",
    "    #Removing columns with more than ACCEPTABLE_NAN_PERCENTAGE of NaN values\n",
    "    mask_nan_columns = [(np.count_nonzero(np.isnan(x_train[:, i]))/x_train.shape[0]) < ACCEPTABLE_NAN_PERCENTAGE for i in range (0, features_number)]\n",
    "    x_train = x_train[:, mask_nan_columns]\n",
    "    features = list(dict.fromkeys(labels))  \n",
    "    features = [features[i]  for i in range (features_number) if mask_nan_columns[i]]\n",
    "    features = {word: index for index, word in enumerate(features)}\n",
    "    #Creating features list\n",
    "    \n",
    "    print(len(features))\n",
    "    #We handle the date and rescale some of the features\n",
    "    x_train = handling_data(x_train, features)\n",
    "\n",
    "    #We remove the features that are not useful\n",
    "    features, x_train = remove_features(x_train, ['WEIGHT2', 'HEIGHT3'], features)\n",
    "   \n",
    "    x_train, features = handle_correlation(x_train, features)\n",
    "   \n",
    "    x_train = apply_pca(x_train)\n",
    "\n",
    "    return x_train\n",
    "\n",
    "def handling_data(x_train, features):\n",
    "    \"\"\"\n",
    "    Handling and modifying data because of special values and scaling some values\n",
    "    :param x_train: training data\n",
    "    :return: modified data\n",
    "    \"\"\"\n",
    "    #Normalizing data\n",
    "    for feature in features.keys() :\n",
    "    \n",
    "        dict_special_value_handle = dictionary_features[feature]\n",
    "\n",
    "\n",
    "        for special_value in dict_special_value_handle.keys() :\n",
    "            #For special values meaning not sure, we change by the median\n",
    "            if special_value == 'dont_know_not_sure' :\n",
    "                x_train[x_train[:, features[feature]] == dict_special_value_handle[special_value], features[feature]] = np.nanmedian(x_train[:, features[feature]])\n",
    "                \n",
    "            #For special values meaning none, we change by 0\n",
    "            elif special_value == 'never' or special_value == 'none'  :\n",
    "                x_train[x_train[:, features[feature]] == dict_special_value_handle[special_value], features[feature]] = 0\n",
    "                    \n",
    "            #For special values meaning refused, we change by NaN\n",
    "            elif special_value == 'refused' :\n",
    "                x_train[x_train[:, features[feature]] == dict_special_value_handle[special_value], features[feature]] = np.nan\n",
    "    \n",
    "    x_train = day_week_month_rescale(x_train, 'STRENGTH', 1, features)\n",
    "    x_train = day_week_month_rescale(x_train, 'ALCDAY5', 1, features)\n",
    "    x_train = day_week_month_rescale(x_train, 'FRUIT1', 2, features)\n",
    "    x_train = day_week_month_rescale(x_train, 'FVBEANS', 2, features)\n",
    "    x_train = day_week_month_rescale(x_train, 'FVORANG', 2, features)\n",
    "    x_train = day_week_month_rescale(x_train, 'FVGREEN', 2, features)\n",
    "    x_train = day_week_month_rescale(x_train, 'VEGETAB1', 2, features)\n",
    "    x_train = day_week_month_rescale(x_train, 'FRUITJU1', 2, features)\n",
    "\n",
    "    return x_train\n",
    "\n",
    "def day_week_month_rescale(x, feature_name, scaling_mode, features):\n",
    "    \"\"\"\n",
    "    Rescale the values of the feature_name\n",
    "    :param x: training data\n",
    "    :param feature_name: feature name\n",
    "    :param scaling_mode: scaling mode\n",
    "    :param features: features list\n",
    "    :return: modified data\n",
    "    \"\"\"\n",
    "\n",
    "    mask_three_hundred = x[:, features[feature_name]] > 300\n",
    "    mask_two_hundred = (x[:, features[feature_name]] >= 200) & (x[:, features[feature_name]] < 300)\n",
    "    mask_one_hundred = (x[:, features[feature_name]] >= 100) &(x[:, features[feature_name]] < 200) \n",
    "        \n",
    "    if scaling_mode == 1 : \n",
    "        x[mask_one_hundred , features[feature_name]] = (x[mask_one_hundred, features[feature_name]] -100)*4.33\n",
    "        x[mask_two_hundred, features[feature_name]] = (x[mask_two_hundred, features[feature_name]] -200)\n",
    "        \n",
    "    elif scaling_mode == 2:\n",
    "        x[x[:, features[feature_name]] == 300, features[feature_name]] = 0\n",
    "        x[mask_one_hundred , features[feature_name]] = (x[mask_one_hundred, features[feature_name]] -100)*(4.33*7)\n",
    "        x[mask_two_hundred, features[feature_name]] = (x[mask_two_hundred, features[feature_name]] -200)*(4.33)\n",
    "        x[mask_three_hundred, features[feature_name]] = (x[mask_three_hundred, features[feature_name]] -300)\n",
    "\n",
    "    return x\n",
    "\n",
    "def handle_correlation(x_train, features):\n",
    "    \"\"\"\n",
    "    Handling correlation between features\n",
    "    :param x_train: training data\n",
    "    :param features: features list\n",
    "    :return: modified and correlation handled data\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace NaN in categorical features with the median value\n",
    "    for feature in category_features['continuous']:\n",
    "        if feature in features.keys() :\n",
    "            median_value = np.nanmedian(x_train[:, features[feature]])\n",
    "            x_train[: ,features[feature]] = np.nan_to_num(x_train[:,features[feature]], nan = median_value)\n",
    "\n",
    "    # Replace NaN in categorical features with the most frequent (mode) value\n",
    "    for feature in category_features['categorical']:\n",
    "    \n",
    "        if feature in features.keys() :\n",
    "            values, counts = np.unique(x_train[:, features[feature]], return_counts=True)\n",
    "            most_represented_class = np.argmax(counts)\n",
    "            x_train[:,features[feature]] = np.nan_to_num(x_train[:,features[feature]], nan = most_represented_class)\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    no_nan_in_row_mask = ~np.isnan(x_train).any(axis=1)\n",
    "    x_train_no_nan = x_train[no_nan_in_row_mask, :]\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    features_correlation = np.corrcoef(x_train_no_nan, rowvar=False)\n",
    "\n",
    "    # Find the features that are highly correlated\n",
    "    correlation_limit = 0.7\n",
    "    correlation_tuple_list = []\n",
    "    correlation_list = []\n",
    "\n",
    "    for i in range(x_train_no_nan.shape[1]) : \n",
    "        for j in range(i, x_train_no_nan.shape[1]) : \n",
    "            if i != j and features_correlation[i,j] >= correlation_limit : \n",
    "                correlation_tuple_list.append((find_key_by_value(features, i) , find_key_by_value(features, j)))\n",
    "                correlation_list.append(find_key_by_value(features, i))\n",
    "                correlation_list.append(find_key_by_value(features, j))\n",
    "    \n",
    "    # Use np.unique to get counts of each element\n",
    "    correlation_list = np.array(correlation_list)\n",
    "    unique_elements, counts = np.unique(correlation_list, return_counts=True)\n",
    "    count_elements = dict(zip(unique_elements, counts))\n",
    "\n",
    "    features_to_remove = set()\n",
    "    # Iterate through the correlation tuples\n",
    "    for feature1, feature2 in correlation_tuple_list:\n",
    "        # Compare the counts of the two features\n",
    "        if count_elements[feature1] > count_elements[feature2]:\n",
    "            features_to_remove.add(feature2)\n",
    "        else:\n",
    "            features_to_remove.add(feature1)\n",
    "\n",
    "    # Remove the features from the features dictionary and x_train_modified\n",
    "    features, x_train = remove_features(x_train, list(features_to_remove), features)\n",
    "\n",
    "    # Update the features dictionary to reflect the new indices\n",
    "    features = {feature: i for i, feature in enumerate(features.keys())}\n",
    "\n",
    "    return x_train, features\n",
    "\n",
    "def apply_pca(x_train):\n",
    "    \"\"\"\n",
    "    Apply PCA to the data\n",
    "    :param x_train: training data\n",
    "    :return: pca applied data\n",
    "    \"\"\"\n",
    "    mean = np.nanmean(x_train, axis=0)\n",
    "    x_tilde = x_train - mean\n",
    "    print(x_tilde)\n",
    "    cov_matrix = np.cov(x_tilde, rowvar=False)\n",
    "    print(cov_matrix)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "    print(eigvals)\n",
    "    eigvals = eigvals[::-1]\n",
    "    eigvecs = eigvecs[:, ::-1]\n",
    "\n",
    "\n",
    "    num_dimensions = 35\n",
    "    W = eigvecs[:, 0 : num_dimensions]\n",
    "    eg = eigvals[0 : num_dimensions]\n",
    "\n",
    "    x_pca = np.dot(x_tilde, W)\n",
    "    return x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "[('_STATE', '_STSTR'), ('FMONTH', 'IDATE'), ('FMONTH', 'IMONTH'), ('IDATE', 'IMONTH'), ('SEQNO', '_PSU'), ('GENHLTH', '_RFHLTH'), ('BLOODCHO', '_CHOLCHK'), ('ASTHMA3', '_ASTHMS1'), ('HAVARTH3', '_DRDXAR1'), ('HAVARTH3', '_LMTACT1'), ('HAVARTH3', '_LMTWRK1'), ('HAVARTH3', '_LMTSCL1'), ('EDUCA', '_EDUCAG'), ('CHILDREN', '_CHLDCNT'), ('INCOME2', '_INCOMG'), ('SMOKE100', '_SMOKER3'), ('FRUITJU1', 'FTJUDA1_'), ('FRUIT1', 'FRUTDA1_'), ('FRUIT1', '_FRUTSUM'), ('FVBEANS', 'BEANDAY_'), ('FVGREEN', 'GRENDAY_'), ('FVGREEN', '_VEGESUM'), ('FVORANG', 'ORNGDAY_'), ('VEGETAB1', 'VEGEDA1_'), ('VEGETAB1', '_VEGESUM'), ('EXERANY2', '_TOTINDA'), ('EXERANY2', '_PACAT1'), ('EXERANY2', '_PA150R2'), ('EXERANY2', '_PA300R2'), ('STRENGTH', 'STRFREQ_'), ('SEATBELT', '_RFSEAT2'), ('SEATBELT', '_RFSEAT3'), ('HIVTST6', '_AIDTST3'), ('_STRWT', '_WT2RAKE'), ('_WT2RAKE', '_LLCPWT'), ('_LTASTH1', '_CASTHM1'), ('_DRDXAR1', '_LMTACT1'), ('_DRDXAR1', '_LMTWRK1'), ('_DRDXAR1', '_LMTSCL1'), ('_PRACE1', '_MRACE1'), ('_MRACE1', '_RACE_G1'), ('_RACE', '_RACEG21'), ('_RACE', '_RACEGR3'), ('_RACE', '_RACE_G1'), ('_RACEG21', '_RACEGR3'), ('_RACEG21', '_RACE_G1'), ('_RACEGR3', '_RACE_G1'), ('_AGEG5YR', '_AGE65YR'), ('_AGEG5YR', '_AGE80'), ('_AGEG5YR', '_AGE_G'), ('_AGE65YR', '_AGE80'), ('_AGE80', '_AGE_G'), ('HTIN4', 'HTM4'), ('WTKG3', '_BMI5'), ('WTKG3', '_BMI5CAT'), ('_BMI5', '_BMI5CAT'), ('_BMI5CAT', '_RFBMI5'), ('FRUTDA1_', '_FRUTSUM'), ('GRENDAY_', '_VEGESUM'), ('VEGEDA1_', '_VEGESUM'), ('_MISFRTN', '_FRUITEX'), ('_TOTINDA', '_PACAT1'), ('_TOTINDA', '_PA150R2'), ('_TOTINDA', '_PA300R2'), ('MAXVO2_', 'FC60_'), ('_PACAT1', '_PAINDX1'), ('_PACAT1', '_PA150R2'), ('_PACAT1', '_PA300R2'), ('_PACAT1', '_PA30021'), ('_PACAT1', '_PAREC1'), ('_PAINDX1', '_PA150R2'), ('_PAINDX1', '_PA300R2'), ('_PAINDX1', '_PA30021'), ('_PAINDX1', '_PAREC1'), ('_PA150R2', '_PA300R2'), ('_PA150R2', '_PAREC1'), ('_PA300R2', '_PA30021'), ('_PA300R2', '_PAREC1'), ('_PASTRNG', '_PASTAE1'), ('_LMTACT1', '_LMTWRK1'), ('_LMTACT1', '_LMTSCL1'), ('_LMTWRK1', '_LMTSCL1'), ('_RFSEAT2', '_RFSEAT3')]\n",
      "[[ 4.58333917e+00  1.50376522e+00 -2.47581026e-02 ...  5.24848005e-01\n",
      "  -1.44940345e-02  3.88772914e-01]\n",
      " [ 5.58333917e+00  5.03765219e-01 -2.47581026e-02 ...  5.24848005e-01\n",
      "  -1.01449403e+00 -6.11227086e-01]\n",
      " [ 3.58333917e+00  5.50376522e+00 -2.47581026e-02 ... -4.75151995e-01\n",
      "  -1.44940345e-02  3.88772914e-01]\n",
      " ...\n",
      " [ 3.58333917e+00  5.50376522e+00 -2.47581026e-02 ...  5.24848005e-01\n",
      "   9.85505966e-01  3.88772914e-01]\n",
      " [ 5.58333917e+00  1.55037652e+01 -2.47581026e-02 ...  5.24848005e-01\n",
      "  -1.44940345e-02  3.88772914e-01]\n",
      " [ 2.58333917e+00 -2.49623478e+00 -2.47581026e-02 ... -4.75151995e-01\n",
      "  -1.44940345e-02  3.88772914e-01]]\n",
      "[[ 1.21992257e+01 -1.56612456e+00 -1.32948590e-01 ... -3.73757549e-02\n",
      "  -2.15906882e-02 -8.93713691e-03]\n",
      " [-1.56612456e+00  6.94638078e+01 -3.82234434e-02 ...  2.99807549e-01\n",
      "  -2.51656618e-01 -2.64149186e-01]\n",
      " [-1.32948590e-01 -3.82234434e-02  2.41452125e-02 ...  2.65704008e-03\n",
      "  -1.94356432e-03 -2.16255813e-03]\n",
      " ...\n",
      " [-3.73757549e-02  2.99807549e-01  2.65704008e-03 ...  7.31863055e-01\n",
      "  -2.36483354e-02 -1.96500818e-02]\n",
      " [-2.15906882e-02 -2.51656618e-01 -1.94356432e-03 ... -2.36483354e-02\n",
      "   2.07150330e-01  6.57688748e-02]\n",
      " [-8.93713691e-03 -2.64149186e-01 -2.16255813e-03 ... -1.96500818e-02\n",
      "   6.57688748e-02  2.37629259e-01]]\n",
      "[12.19922567         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan]\n"
     ]
    }
   ],
   "source": [
    "x_train_cleaned = clean_data_x(x_train, labels)\n",
    "\n",
    "\n",
    "np.savetxt(\"x_train_cleaned.csv\", x_train_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
