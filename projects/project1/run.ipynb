{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from cleaning_data import *\n",
    "from stats import *\n",
    "from functions import *\n",
    "from clean_data_testing import *\n",
    "from utils import split_data, downsample_class_0, upsample_class_1_to_percentage\n",
    "from functions import *\n",
    "from clean_data_testing import *\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m DATA_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUNIV\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMASTER-EPFL\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMachine-Learning\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mML_course\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mprojects\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mproject1\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m x_train, x_test, y_train, train_ids, test_ids, labels \u001b[38;5;241m=\u001b[39m  load_csv_data(DATA_PATH, sub_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m labels\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\UNIV\\MASTER-EPFL\\Machine-Learning\\ML_course\\projects\\project1\\helpers.py:35\u001b[0m, in \u001b[0;36mload_csv_data\u001b[1;34m(data_path, sub_sample)\u001b[0m\n\u001b[0;32m     25\u001b[0m    labels \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadline()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     28\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(\n\u001b[0;32m     29\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     30\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     usecols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 35\u001b[0m x_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(\n\u001b[0;32m     36\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, skip_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(\n\u001b[0;32m     39\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, skip_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m train_ids \u001b[38;5;241m=\u001b[39m x_train[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ada\\Lib\\site-packages\\numpy\\lib\\npyio.py:2327\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;66;03m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;66;03m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loose:\n\u001b[0;32m   2326\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 2327\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[conv\u001b[38;5;241m.\u001b[39m_loose_call(_r) \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2328\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2330\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2331\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[conv\u001b[38;5;241m.\u001b[39m_strict_call(_r) \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2332\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ada\\Lib\\site-packages\\numpy\\lib\\npyio.py:2327\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;66;03m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;66;03m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loose:\n\u001b[0;32m   2326\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 2327\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[conv\u001b[38;5;241m.\u001b[39m_loose_call(_r) \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2328\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2330\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2331\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[conv\u001b[38;5;241m.\u001b[39m_strict_call(_r) \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2332\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ada\\Lib\\site-packages\\numpy\\lib\\npyio.py:2327\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;66;03m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;66;03m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loose:\n\u001b[0;32m   2326\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m-> 2327\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[conv\u001b[38;5;241m.\u001b[39m_loose_call(_r) \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2328\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2330\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2331\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[[conv\u001b[38;5;241m.\u001b[39m_strict_call(_r) \u001b[38;5;28;01mfor\u001b[39;00m _r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2332\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m (i, conv) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ada\\Lib\\site-packages\\numpy\\lib\\_iotools.py:672\u001b[0m, in \u001b[0;36mStringConverter._loose_call\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_loose_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 672\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(value)\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'D:\\\\UNIV\\\\MASTER-EPFL\\\\Machine-Learning\\\\ML_course\\\\projects\\\\project1\\\\data\\\\dataset\\\\dataset'\n",
    "x_train, x_test, y_train, train_ids, test_ids, labels =  load_csv_data(DATA_PATH, sub_sample=False)\n",
    "labels.pop(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing\n",
    "x_tr, x_te, y_tr, y_te = split_data(x_train, y_train, 0.8, seed= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For simplicty, replace the label -1 to 0\n",
    "y_tr[y_tr == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Up sample the data\n",
    "x_tr_up, y_tr_up = upsample_class_1_to_percentage(x_tr, y_tr, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Down sample the data\n",
    "x_tr_ds, y_tr_ds = downsample_class_0(x_tr, y_tr, down_sampling_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain 90.0% variance: 63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_cleaned, features, median_and_most_probable_class, W, mean = clean_train_data(x_tr_up, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te_cleaned = clean_test_data(x_te, labels, features, median_and_most_probable_class, mean, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cleaned = clean_test_data(x_test, labels, features, median_and_most_probable_class, mean, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65627, 63)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342081, 63)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 100000\n",
    "gamma = 0.001\n",
    "batch_size = 64\n",
    "lambda_ =  0.005\n",
    "num_samples = x_train_cleaned.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# Parameters for the Gaussian distribution\n",
    "mean = 0    # Mean of the distribution\n",
    "std_dev = 1 # Standard deviation of the distribution\n",
    "\n",
    "tx = np.c_[np.ones(num_samples), x_train_cleaned]\n",
    "w_initial = np.random.normal(loc=mean, scale=std_dev, size=tx.shape[1])\n",
    "\n",
    "\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "y_tr_up, tx, w_initial, batch_size, max_iters, gamma, lambda_\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = x_te.shape[0]\n",
    "tx_te = np.c_[np.ones(num_samples), x_te_cleaned]\n",
    "y_predict = predict(tx_te, sgd_ws[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.801057491581209"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_te, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.38181818181818183, 0.6806212018906145)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_te, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = x_test_cleaned.shape[0]\n",
    "tx_test = np.c_[np.ones(num_samples), x_test_cleaned]\n",
    "y_test_to_save = predict(tx_test, sgd_ws[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"'D:\\\\UNIV\\\\MASTER-EPFL\\\\Machine-Learning\\\\ML_course\\\\projects\\\\project1\\\\data\\\\dataset\\\\dataset\\\\sample_submission.csv\", y_test_to_save, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the ids and predictions together column-wise\n",
    "submit = np.column_stack((test_ids, y_test_to_save))\n",
    "\n",
    "# Save to a CSV file using np.savetxt\n",
    "np.savetxt(\"'D:\\\\UNIV\\\\MASTER-EPFL\\\\Machine-Learning\\\\ML_course\\\\projects\\\\project1\\\\data\\\\dataset\\\\dataset\\\\sample_submission.csv\", submit, delimiter=\",\", fmt='%d,%d', header='Id,Prediction', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
